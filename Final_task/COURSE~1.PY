# -*- coding: utf-8 -*-
"""Course_project_Python_for_Data_Science

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9smN6pSxT5ouSfhjBPsunYr9HZCzRGT

## Курсовой проект для курса "Библиотеки Python для Data Science: Numpy, Matplotlib, Scikit-learn"

### Задание:
Используя данные из train.csv, построить модель для предсказания цен на недвижимость (квартиры). С помощью полученной модели предсказать цены для квартир из файла test.csv.

### Поля данных:
<p/>Id - идентификационный номер квартиры
<p/>DistrictId - идентификационный номер района
<p/>Rooms - количество комнат
<p/>Square - площадь
<p/>LifeSquare - жилая площадь
<p/>KitchenSquare - площадь кухни
<p/>Floor - этаж
<p/>HouseFloor - количество этажей в доме
<p/>HouseYear - год постройки дома
<p/>Ecology_1, Ecology_2, Ecology_3 - экологические показатели местности
<p/>Social_1, Social_2, Social_3 - социальные показатели местности
<p/>Healthcare_1, Helthcare_2 - показатели местности, связанные с охраной здоровья
<p/>Shops_1, Shops_2 - показатели, связанные с наличием магазинов, торговых центров
<p/>Price - цена квартиры

#### Примечание: данный файл доступен в Google Colab по ссылке: https://colab.research.google.com/drive/1P9smN6pSxT5ouSfhjBPsunYr9HZCzRGT?usp=sharing

### Загрузка исходных данных
"""

import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, pointbiserialr, pearsonr
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Открытие CSV файла в DataFrame
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GB_Python_libraries_for_Data_Science/Course_project_Python_for_Data_Science/test.csv')
train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GB_Python_libraries_for_Data_Science/Course_project_Python_for_Data_Science/train.csv')

# Вывод первых нескольких строк test DataFrame
print(test.head())

# Вывод первых нескольких строк train DataFrame
print(train.head())

# Вывод первых нескольких строк train DataFrame
print(train.head())

"""### Проверка и обработка дублирующихся строк в обучающем датасете."""

duplicates = train.duplicated()
print("Дублирующиеся строки:")
print(train[duplicates])

#### Вывод: повторяющихся строк в обучающем датафрейме нет.

"""### Просмотр гистограмм переменных в обучающем наборе данных"""

train.hist(figsize=(16,16), bins=20, grid=False)

### Вывод: есть нулевые значения в переменных Square, LifeSquare, KitchenSquare, HouseFloor - это явные ошибки
### и строки с ними нужно удалить

# Удаление нулевых значений в тестовом (если в них есть нули) и тренеровочным наборах данных
train = train[(train['Square'] != 0) &
                    (train['LifeSquare'] != 0) &
                    (train['KitchenSquare'] != 0) &
                    (train['HouseFloor'] != 0)]

test = test[(test['Square'] != 0) &
                  (test['LifeSquare'] != 0) &
                  (test['KitchenSquare'] != 0) &
                  (test['HouseFloor'] != 0)]

"""### Просмотр форматов данных и наличия пропусков"""

train.info()

"""Вывод: много пропущенных значений в переменной LifeSquare (жилая площадь) и в переменной Healthcare_1 (показатели местности, связанные с охраной здоровья). Необходимо рассмотреть эти переменные подробнее."""

### Описательная статистика переменной LifeSquare (жилая площадь)
print(f"Кол-во нечисловых значений: {train['LifeSquare'].isna().sum()}")
train['LifeSquare'].describe().round(2)

# Отрисовка гистограммы распределения переменной LifeSquare
plt.figure(figsize=(8, 6))
sns.histplot(train['LifeSquare'], kde=True)
plt.title('Гистограмма LifeSquare')
plt.xlabel('Жилая площадь')
plt.ylabel('Частота встречаемости даной площади')
plt.show()

#### Вывод: распределение ненормальное с сильной положительной ассиметрией свидетельствующей, вероятно, о выбросах в данных.

# Отрисовка графика "Ящик с усами"
plt.figure(figsize=(8, 6))
sns.boxplot(data=train, y='LifeSquare')
plt.title('LifeSquare')
plt.ylabel('Жилая площадь')
plt.show()

### Вывод: выбросы в переменной LifeSquare есть в большом количестве, также есть одно сильно выделяющееся значение

### Учитывая тип распределения этой переменной и её важность, лучше заменить её пропущенные значения медианными значениями.

### Выбор столбцов для заполнения пропущенных значений медианными значениями

columns_to_median = ['LifeSquare']

for column in columns_to_median:
    median_rating = train[column].median()
    print(median_rating)
    train[column] = train[column].fillna(median_rating)

### Заполнение медианным значением тестового набора данных (если есть пропуски)
test['LifeSquare'].fillna(test['LifeSquare'].median(), inplace=True)

### Проверка самого большого выброса данных в переменной LifeSquare
train[train['LifeSquare'] > 7000]

### Вывод: это, явно, ошибочное значение, т.к. жилая площадь не может быть больше общей площади
### Следовательно, эту запись можно удалить из датасета

### Удаляем выбросы по показателю LifeSquare
train = train[train['LifeSquare'] <= 7000]

# Отрисовка графика "Ящик с усами"
plt.figure(figsize=(8, 6))
sns.boxplot(data=train, y='LifeSquare')
plt.title('LifeSquare')
plt.ylabel('Жилая площадь')
plt.show()

### Вывод: выбросы в переменной LifeSquare по прежнему есть есть в большом количестве

### Проверка оставихся выбросов данных в переменной LifeSquare
train[train['LifeSquare'] > 400]

### Вывод: попрежнему есть ошибочные значения
### Следовательно, нужно удалить все записи, в которых жилая площадь больше общей площади

### Определяем ошибочные записи по показателю LifeSquare
indixes_to_drop = train[train['LifeSquare'] >= train['Square']].index
print(train.loc[indixes_to_drop])

### Вывод: таких записей всего 439 - их можно удалить

### Удаляем ошибочные записи по показателю LifeSquare
train = train.drop(indixes_to_drop)

# Отрисовка графика "Ящик с усами"
plt.figure(figsize=(8, 6))
sns.boxplot(data=train, y='LifeSquare')
plt.title('LifeSquare')
plt.ylabel('Жилая площадь')
plt.show()

### Вывод: количество выбросов в переменной LifeSquare уменьшилось

"""### Работа с пропущенными значениями переменной Healthcare_1 в обучающем наборе данных"""

### Описательная статистика переменной Healthcare_1
print(f"Кол-во нечисловых значений: {train['Healthcare_1'].isna().sum()}")
train['Healthcare_1'].describe().round(2)

# Отрисовка гистограммы распределения переменной Healthcare_1
plt.figure(figsize=(8, 6))
sns.histplot(train['Healthcare_1'], kde=True)
plt.title('Гистограмма Healthcare_1')
plt.xlabel('Здравоохранение в районе')
plt.ylabel('Частота встречаемости данного показателя')
plt.show()

#### Вывод: распределение ненормальное с сильной положительной ассиметрией свидетельствующей, вероятно, о выбросах в данных.

# Отрисовка графика "Ящик с усами"
plt.figure(figsize=(8, 6))
sns.boxplot(data=train, y='Healthcare_1')
plt.title('Healthcare_1')
plt.ylabel('Здравоохранение в районе')
plt.show()

### Вывод: выбросов в переменной Healthcare_1 не очень много

### Просмотр всех выбросов в переменной Healthcare_1

column_name = 'Healthcare_1'

# Расчет Q1 (25-й процентиль) и Q3 (75-й процентиль)
Q1 = train[column_name].quantile(0.25)
Q3 = train[column_name].quantile(0.75)

# Расчет межквартильного интервала
IQR = Q3 - Q1

# Определение границ выбросов
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# выбор поднабора данных только с выбросами
filtered_train = train[(train[column_name] < lower_bound) | (train[column_name] > upper_bound)]

# Распечатка строк  выбросами
print("Выбросы из обучающего набора данных по переменной Healthcare_1:".format(column_name))
print(filtered_train)

### Вывод: хотя выбросов не та уж много - всего 210 из около 5000 значений, но очень много незаполненных значений, поэтому эту переменную удаляем из обучающего набора данных

# Удаляем Healthcare_1 из обучающего набора данных
train = train.drop(columns=['Healthcare_1'])
print(train.columns)

# Удаляем Healthcare_1 из тестового набора данных
test = test.drop(columns=['Healthcare_1'])
print(test.columns)

"""### Работа с целевой переменной Price"""

### Описательная статистика переменной Price
print(f"Кол-во нечисловых значений: {train['Price'].isna().sum()}")
train['Price'].describe().round(2)

# Отрисовка гистограммы распределения переменной Price
plt.figure(figsize=(8, 6))
sns.histplot(train['Price'], kde=True)
plt.title('Гистограмма Price')
plt.xlabel('Цена недвижимости')
plt.ylabel('Частота встречаемости данного показателя')
plt.show()

#### Вывод: распределение ненормальное с небольшой положительной ассиметрией свидетельствующей, вероятно, о выбросах в данных.

# Отрисовка графика "Ящик с усами"
plt.figure(figsize=(8, 6))
sns.boxplot(data=train, y='Price')
plt.title('Price')
plt.ylabel('Цена недвижимости')
plt.show()

### Вывод: выбросов в переменной Price много, но определить ошибочны ли они на данном этапе нельзя, т.к. это могут быть, действительно, дорогие дома.

# Проверка всех вариантов значений в объектных столбцах датафрейма
for col in train.columns:
    if train[col].dtype == 'object':
        unique_values = train[col].unique()
        print(f"Уникальные значения в столбце '{col}': {unique_values}")

"""### Применяем one-hot encoding для переменных Ecology_2, Ecology_3, Shops_2"""

train = pd.get_dummies(train, columns=['Ecology_2', 'Ecology_3', 'Shops_2'])

# Распечатка получившегося набора данных
print("\nНабор данных после one-hot encoding:")
print(train.head())

test = pd.get_dummies(test, columns=['Ecology_2', 'Ecology_3', 'Shops_2'])

# Распечатка получившегося набора данных
print("\nНабор данных после one-hot encoding:")
print(test.head())

"""### Создание новых переменных на основании года постройки дома"""

train['HouseAge'] = 2024 - train['HouseYear']
test['HouseAge'] = 2024 - test['HouseYear']

train.info()

test.info()

"""### Исследование корреляций между целевой переменной и другими переменными"""

### Поскольку все переменные на нормальное распределение мы не проверяли, то лучше исользовать для оценки корреляции ранговый коэффициент Спирмена

variable = 'Price'

# Выбор только количественных переменных
quantitative_vars = train.select_dtypes(include=['int64', 'float64'])

# Рассчет коэффициента корреляции Спирмена и p-value
results = {}
for col in quantitative_vars.columns:
    if col != variable:
        spearman_corr, p_value = spearmanr(train[variable], train[col])
        results[col] = {'Spearman_corr': spearman_corr, 'p_value': p_value}

results_df = pd.DataFrame.from_dict(results, orient='index')

print("Коэффициенты корреляции Спирмена и их р-значения с переменной ", variable, ":")
print(results_df)

#### выбор переменных с коэффициентом корреляции Спирмена больше 0,5 (корреляция средней силы и выше) с p-значением меньше или равным 0,05 (значимые отличия от нуля)

significant_correlations = {col: values for col, values in results.items() if values['Spearman_corr'] > 0.5 and values['p_value'] <= 0.05}

significant_correlations_df = pd.DataFrame.from_dict(significant_correlations, orient='index')

print("Значимые высокие корреляции переменной Price с переменными:")
print(significant_correlations_df)

# При определении взаимосвязи между целевой количественной (непрерывной) переменной и
# несколькими логическими (двоичными) переменными в DataFrame наиболее подходящим коэффициентом корреляции
# является точечный бисериальный коэффициент корреляции.
correlations = {}
target_column = 'Price'

# Выбор только булевых переменных
bool_vars = train.select_dtypes(include=['bool'])

for column in bool_vars.columns:
    if column != target_column:
        corr, p_value = pointbiserialr(train[column], train[target_column])
        correlations[column] = corr

# Print the correlation coefficients
print("Point biserial correlation coefficients:")
for column, corr in correlations.items():
    print(f"{column}: {corr}")

# Вывод: булевы переменные практически не взаимосвязаны с ценами на дома

# Визуализация матрицы корреляции Спирмена для количественных переменных
spearman_corr_matrix = quantitative_vars.corr(method='spearman')

# Plot Spearman correlation heatmap
plt.figure(figsize=(20, 18))
sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt=".1f", linewidths=.5)
plt.title('Корреляция Спирмана всех количественных переменных изучаемого набора данных')
plt.show()

#### Вывод: переменными с наибольшей корреляцией являются:
# 1. количество комнат, площадь и жилая прощадь
# 2. экологические, социальные переменные и наличие магазинов
# 3. следует заметить, что корреляции эти средней силы (0,5 - 0,7), и коррелированные переменные могут взаимозаменять
# друга в будущей модели.

# Для определения взаимосвязи между несколькими логическими переменными
# наиболее подходящим коэффициентом корреляции является коэффициент Phi (φ).

correlation_matrix = bool_vars.corr(method=lambda x, y: pearsonr(x, y)[0])

print("Correlation matrix (Phi coefficient) for Boolean variables:")
print(correlation_matrix)

# Вывод: между собой булевы переменные слабо коррелируют, однако в рамках одной переменной
# (с одним и тем же индексом) переменные имеют сильную обратную корреляцию, т.е. их влияние
# в любой будущей модели прямопротивоположно.

# Отрисовка heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Тепловая карта Фи-коэффициента булевых переменных')
plt.show()

"""### Создание и тренировка модели (для демонстрации была выбрана модель случайного леса, т.к. использование этой модели менее требовательно к масштабированию и нормализации предикторов)"""

# Разделение тренировочного набора данных на предикторы и отклик
X = train.drop(columns=['Id', 'Price'])
y = train['Price']

# Разделение тренировочного набора данных на обучающую и тестовую часть
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)
X_train.shape, X_val.shape, y_train.shape, y_val.shape

# Инициализация и подгонка модели
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Валидация модели на тестовой части
y_pred = model.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)
print(f'Средняя абсолютная ошибка: {mae}')
print(f'Коэффициент детерминации (R²): {r2.round(3)}')

### Вывод: модель объясняет 71,8 % вариабельности предикторов, а 28,2 % - это случайные влияния

# Валидация модели на обучающей части
y_train_pred = model.predict(X_train)
mae = mean_absolute_error(y_train, y_train_pred)
r2 = r2_score(y_train, y_train_pred)
print(f'Средняя абсолютная ошибка: {mae}')
print(f'Коэффициент детерминации (R²): {r2.round(3)}')

# Plot for training data
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_train, y_train_pred, alpha=0.3)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=3)
plt.xlabel('Истинные значения')
plt.ylabel('Прогнозные значения')
plt.title('Тренировочная часть набора данных')

# Plot for validation data
plt.subplot(1, 2, 2)
plt.scatter(y_val, y_pred, alpha=0.3)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=3)
plt.xlabel('Истинные значения')
plt.ylabel('Прогнозные значения')
plt.title('Тестовая часть набора данных')

plt.tight_layout()
plt.show()

"""### Улучшение модели путем кросс-валидации"""

cv_score = cross_val_score(model, X_train, y_train, scoring = 'r2', cv=KFold(n_splits=5, shuffle = True, random_state=42))
cv_score.mean()

#### Вывод: кросс-валидация не улучшила модель

"""### Определение значимости предикторов"""

feature_impotance = pd.DataFrame(zip(X_train.columns, model.feature_importances_),
                                 columns=["Переменная", "Значимость"])

feature_impotance.sort_values(by = "Значимость", ascending=False)

"""### Получение прогноза на тестовом наборе данных"""

# Подготовка тестового набора
X_test = test.drop(columns=['Id'])

# Получение прогноза
test_predictions = model.predict(X_test)

# Создание фрейма данных с прогнозом цен
results = pd.DataFrame({'Id': test['Id'], 'Price': test_predictions.round(2)})

results.head()

# Сохранение результата в CSV файл
results.to_csv('AOmelchenko_predictions.csv', index=False)

# Сохранение модели в файл
with open('RF_model_22-06-2024.pickle', 'wb') as file:
  pickle.dump(model, file)