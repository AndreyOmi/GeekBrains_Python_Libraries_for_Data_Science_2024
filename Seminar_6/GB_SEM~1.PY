# -*- coding: utf-8 -*-
"""GB_seminar_06-06-2024

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1PRnFQdOwjiPKBfzsX80YClfyKV9P-D

## Практическое задание к семинару № 6 от 06-06-2024

## Тема “Обучение с учителем”

### Задание 1
Импортируйте библиотеки pandas и numpy. Загрузите "Boston House Prices dataset" из встроенных наборов данных библиотеки sklearn. Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки составлял 30% от всех данных, при этом аргумент random state должен быть равен 42. Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.
"""

import pandas as pd
import numpy as np
from keras.datasets import boston_housing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import r2_score, roc_auc_score
import matplotlib.pyplot as plt

# по этическим причинам (так написано на сайте модуля sklearn) в последней версии sklearn удален набор цен на дома в Бостоне,
# поэтому я загрузил этот датасет из модуля keras - датасет при загрузке автоматически разделяется на 20% тестового и
# 80% обучающего набора данных
(X, y), (X_test_unused, y_test_unused) = boston_housing.load_data()

# Разделение данных на выборки в соотношение 30% к 70% вручную
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Размеры датасетов
print(f"x_train shape: {x_train.shape}")
print(f"x_test shape: {x_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Создание линейной модели
lr = LinearRegression()

# Обучение модели
lr.fit(x_train, y_train)

# Получение прогноза
y_pred_lr = lr.predict(x_test)

# Расчет метрики R² (коэффициент детерминации)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"R²: {r2_lr}")

# Отрисовка графика реальных и прогнозных значений
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_lr, color='blue', edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Реальные значения')
plt.ylabel('Модельные значения')
plt.title('Реальные и модельные значения линейной регрессии')
plt.show()

"""### Задание 2
Создайте модель под названием model с помощью класса RandomForestRegressor из модуля sklearn.ensemble. Сделайте агрумент n_estimators равным 1000, max_depth должен быть равен 12 и random_state сделайте равным 42. Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression, но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0], чтобы получить из датафрейма одномерный массив Numpy, так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно
применение массивов вместо датафрейма. Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания. Напишите в комментариях к коду, какая модель в данном случае работает лучше.
"""

# Создание модели случайного леса
model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)

# Обучение модели
model.fit(x_train, y_train)

# Получение прогнозных значений
y_pred_rf = model.predict(x_test)

# Расчет метрики R²
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Метрика R² для модели случайного леса: {r2_rf}")

# Сравнение моделей
if r2_rf > r2_lr:
    print("Модель Random Forest Regressor лучше.")
else:
    print("Модель Linear Regression лучше.")

# График реальных и прогнозных значений для линейной регрессии
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_lr, color='blue', edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Реальные значения')
plt.ylabel('Прогнозные значения')
plt.title('Реальные и модельные значения линейной регрессии')

# График реальных и прогнозных значений для модели случайного леса
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred_rf, color='green', edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Реальные значения')
plt.ylabel('Прогнозные значения')
plt.title('Реальные и модельные значения случайного леса')

plt.tight_layout()
plt.show()

"""### *Задание 3
Вызовите документацию для класса RandomForestRegressor, найдите информацию об атрибуте feature_importances_. С помощью этого атрибута найдите сумму всех показателей важности, установите, какие два признака показывают наибольшую важность.
"""

### В модуле keras нет заголовков в наборе данных ,
### поэтому названия и описание столбцов я взял из описания этого датасета

# The Boston Housing dataset contains 13 features:

    # CRIM: per capita crime rate by town
    # ZN: proportion of residential land zoned for lots over 25,000 sq. ft.
    # INDUS: proportion of non-retail business acres per town
    # CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)
    # NOX: nitric oxides concentration (parts per 10 million)
    # RM: average number of rooms per dwelling
    # AGE: proportion of owner-occupied units built prior to 1940
    # DIS: weighted distances to five Boston employment centers
    # RAD: index of accessibility to radial highways
    # TAX: full-value property tax rate per $10,000
    # PTRATIO: pupil-teacher ratio by town
    # B: 1000(Bk - 0.63)^2 where Bk is the proportion of Black people by town
    # LSTAT: % lower status of the population

    # MEDV: The target variable is the median value of owner-occupied homes in $1000's.

# Получение важных предикторов
importances = model.feature_importances_

# суммирование вклада важных предикторов - для проверки
total_importance = np.sum(importances)
print(f"Sum of all feature importances: {total_importance}")

# Получение и сортировка индексов важных предикторов
most_important_indices = importances.argsort()[-2:][::-1]

column_names = [
    "CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX",
    "PTRATIO", "B", "LSTAT", "MEDV"
]

# Распечатка названий и оценок значимости предикторов
for idx in most_important_indices:
    print(f"Feature: {column_names[idx]}, Importance: {importances[idx]}")

"""### *Задание 4
В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакции по кредитной карте являются
мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.
<p/>Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.
<p/>Загрузите датасет creditcard.csv и создайте датафрейм df.
<p/>С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована. Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:
<p/>pd.options.display.max_columns = 100.
<p/>Просмотрите первые 10 строк датафрейма df.
<p/>Создайте датафрейм X из датафрейма df, исключив столбец Class.Создайте объект Series под названием y из столбца Class. Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y. У вас должны получиться объекты X_train, X_test, y_train и y_test.
<p/>Просмотрите информацию о их форме.
<p/>Для поиска по сетке параметров задайте такие параметры:
<p/>parameters = [{'n_estimators': [10, 15],
<p/>'max_features': np.arange(3, 5),
<p/>'max_depth': np.arange(4, 7)}]
<p/>Создайте модель GridSearchCV со следующими аргументами:
<p/>estimator=RandomForestClassifier(random_state=100),
<p/>param_grid=parameters,
<p/>scoring='roc_auc',
<p/>cv=3.
<p/>Обучите модель на тренировочном наборе данных (может занять несколько минут). Просмотрите параметры лучшей модели с помощью атрибута best_params_.
<p/>Предскажите вероятности классов с помощью полученной модели и метода predict_proba.
<p/>Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.
<p/>Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba.
"""

# Загрузка датасета
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GB_Python_libraries_for_Data_Science/creditcard.csv')

# Проверка несбалансированности набора данных
print(df['Class'].value_counts(normalize=True))

# Проверка форматов и пропусков в данных - пропусков нет
print(df.info())

# Установка на просмотр всех столбцов
pd.options.display.max_columns = 100

# Просмотр шапки и первых 10 строк
print(df.head(10))

# Создание dataframe X из столбца Class
X = df.drop(columns='Class')

# Создание типа данных Series
y = df['Class']

# Разделение выборок
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)

# Просмотр размеров выборок
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Определение параметров сетки
parameters = [
    {'n_estimators': [10, 15],
     'max_features': np.arange(3, 5),
     'max_depth': np.arange(4, 7)}
]

# Создание модели GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=100),
                           param_grid=parameters,
                           scoring='roc_auc',
                           cv=3)

# Обучение модели
grid_search.fit(X_train, y_train)

# Просмотр лучших параметров
print(grid_search.best_params_)

# Прогнозирование вероятностей используя параметры лучшей модели на тестовых данных
y_pred_proba = grid_search.best_estimator_.predict_proba(X_test)[:, 1]

# Расчет AUC на тестовых данных
test_auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC на тестовых данных: {test_auc}")

# Прогнозирование вероятностей используя параметры лучшей модели на обучающих данных
y_train_pred_proba = grid_search.best_estimator_.predict_proba(X_train)[:, 1]

# Расчет AUC на обучающих данных
train_auc = roc_auc_score(y_train, y_train_pred_proba)
print(f"AUC на обучающих данных: {train_auc}")

"""### *Дополнительные задания:

### 1). Загрузите датасет Wine из встроенных датасетов sklearn.datasets с помощью функции load_wine в переменную data.
"""

from sklearn.datasets import load_wine

# Загрузка набора данных Wine
data = load_wine()

print(data)

"""### 2). Полученный датасет не является датафреймом. Это структура данных, имеющая ключи аналогично словарю. Просмотрите тип данных этой структуры данных и создайте список data_keys, содержащий ее ключи."""

# Проверка типа данных
data_type = type(data)
print(f"Тип данных: {data_type}")

# Создание списка ключей
data_keys = list(data.keys())
print(f"Ключи: {data_keys}")

"""### 3). Просмотрите данные, описание и названия признаков в датасете. Описание нужно вывести в виде привычного, аккуратно оформленного текста, без обозначений переноса строки, но с самими переносами и т.д."""

# Присвоение переменным имен и описаний предикторов
wine_data = data.data
wine_feature_names = data.feature_names
wine_description = data.DESCR

# Форматирование описаний
formatted_description = " ".join(wine_description.split())

# Отображение описаний и имен предикторов
print("Description:\n")
print(formatted_description)

print("\nFeature Names:\n")
print(wine_feature_names)

print("\nШапка и первые строки фрейма данных:\n")
print(pd.DataFrame(wine_data, columns=wine_feature_names).head())

"""### 4). Сколько классов содержит целевая переменная датасета? Выведите названия классов."""

# Сохранение целевой переменной
wine_target = data.target

# Число уникальных классов
unique_classes = np.unique(wine_target)
num_classes = len(unique_classes)

# Печать названий и числа уникальных классов
print(f"Целевая переменная содержит {num_classes} класса.")
print(f"Названия классов: {unique_classes}")

# Печать названия целевой переменной
if 'target_names' in data:
    target_names = data.target_names
    print(f"Название класса целевой переменной: {target_names}")

"""### 5). На основе данных датасета (они содержатся в двумерном массиве Numpy) и названий признаков создайте датафрейм под названием X."""

# Экскракция данных
wine_data = data.data
feature_names = data.feature_names

# Создание фрейма данных Х
X = pd.DataFrame(wine_data, columns=feature_names)

# Отображение первых строк
print(X.head())

"""### 6). Выясните размер датафрейма X и установите, имеются ли в нем пропущенные значения."""

# Определение размера датафрейма X
size_of_X = X.shape
print("Size of DataFrame X:", size_of_X)

# Поиск пропущенных значений
missing_values = X.isna().sum().sum()
if missing_values == 0:
    print("Пропущенных значений нет.")
else:
    print("В данных есть пропущенные значения.")

"""### 7). Добавьте в датафрейм поле с классами вин в виде чисел, имеющих тип данных numpy.int64. Название поля - 'target'."""

wine_target = data.target

# Добавление целевого столбца
X['target'] = pd.Series(wine_target, dtype=np.int64)

# Печать первых строк таблицы
print(X.head())

"""### 8). Постройте матрицу корреляций для всех полей X. Дайте полученному датафрейму название X_corr."""

# Расчет матрицы корреляций
X_corr = X.corr()

# Печать матрицы корреляций
print(X_corr)

"""### 9). Создайте список high_corr из признаков, корреляция которых с полем target по абсолютному значению превышает 0.5 (причем, само поле target не должно входить в этот список)."""

# Установка порога значения
threshold = 0.5

# Инициализация списка для значений корреляций
high_corr = []

# Заполнение списка значениями высокой корреляцией
for feature in X_corr.columns:
    if feature != 'target' and abs(X_corr['target'][feature]) > threshold:
        high_corr.append(feature)

# Печать списка высоких корреляций
print("Кореляции превышающие 0,5:")
print(high_corr)

"""### 10). Удалите из датафрейма X поле с целевой переменной. Для всех признаков, названия которых содержатся в списке high_corr, вычислите квадрат их значений и добавьте в датафрейм X соответствующие поля с суффиксом '_2', добавленного к первоначальному названию признака.Итоговый датафрейм должен содержать все поля, которые, были в нем изначально, а также поля с признаками из списка high_corr, возведенными в квадрат. Выведите описание полей датафрейма X с помощью метода describe."""

# Удаление целевой переменной
X = X.drop(columns=['target'])

# Расчет квадратов высоких корреляций и сохранение их в отдельные столбцы
for feature in high_corr:
    X[f"{feature}_2"] = X[feature] ** 2

# Печать описания столбцов датафрейма
print(X.describe())